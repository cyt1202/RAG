# RAG
新手尝试搭建一个基于文档的问答模型！模型结构如下：

1.文档切割 2. 向量化 3. 存入向量数据库 4. 在向量数据库中检索对应知识 5. 返回给大语言模型的API

## 文档切割
分词应该尽量准确的进行语义分割

我最开始进行了简单粗暴的尝试：基于分隔符的递归切分

根据用  `\n\n `切分整个文档;如果切出来的chunks大于chunk_size,那么只针对过大的chunk进行下一个优先级的分隔符 `\n `切分; 如果用 \n 切分后，某个子块还是太大，就会再对那个子块用空格` " " `切分。如果最后还不行，就会强制按字符 `"" `切分，以确保没有任何一个块会超过 chunk_size

但是这样无法保证语义信息的完整性

我又尝试寻找开源方法，文档切割作为RAG的上游任务，单独的模型较少：huggingface上更多是整个RAG的pipeline
最终选择一下模型进行测试：
1. chonky
  chonky 使用了 HuggingFace 的 Transformers 库，采用 BERT 或更现代的 Bert 变体（如 ModernBERT），进行 Token Classification（分词分类）任务。模型的输出标签只有两类：
"O"：普通词 "separator"：切分点
  使用数据集进行训练，使得模型学会再输入的文本token序列中标记出切分点separator
3. 
