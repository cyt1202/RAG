# RAG
新手尝试搭建一个基于文档的问答模型！模型结构如下：

1.文档切割 2. 向量化 3. 存入向量数据库 4. 在向量数据库中检索对应知识 5. 返回给大语言模型的API

## 文档切割
分词应该尽量准确的进行语义分割

我最开始进行了简单粗暴的尝试：

1. 基于分隔符的递归切分

  根据用  `\n\n `切分整个文档;如果切出来的chunks大于chunk_size,那么只针对过大的chunk进行下一个优先级的分隔符 `\n `切分; 如果用 \n 切分后，某个子块还是太大，就会再对那个子块用空格` " " `切分。如果最后还不行，就会强制按字符 `"" `切分，以确保没有任何一个块会超过 chunk_size
  
  但是这样无法保证语义信息的完整性。我又尝试寻找开源方法，文档切割作为RAG的上游任务，单独的模型较少：huggingface上更多是整个RAG的pipeline
最终选择一下模型进行测试：

2. chonky
  chonky 使用了 HuggingFace 的 Transformers 库，采用 BERT 或更现代的 Bert 变体（如 ModernBERT），进行 Token Classification（分词分类）任务。模型的输出标签只有两类：
"O"：普通词 "separator"：切分点

  使用数据集进行训练，使得模型学会再输入的文本token序列中标记出切分点separator
  
3. semantic-text-splitter
   3.1 在不超过指定长度（chunk_size）的前提下，尽可能让每个“区块”（chunk）包含最完整、最独立的语义单元

   3.2 分割器首先有一套预设的规则，定义了文本中不同元素的“重要性”或“层级”：标题 (Heading) > 段落 (Paragraph) > 句子 (Sentence) > 词 (Word) > 字符 (Character)

   3.3 对于MarkdownSplitter：这个层级非常清晰，可以直接利用了 Markdown 的语法结构；于 TextSplitter：它处理的是纯文本，所以使用换行符来模拟结构：最高级：连续的多个换行符（如 \n\n，通常代表段落分隔），次高级：单个换行符 (\n)，然后是：句子，最低级：单词和字符

   3.4 “ 自顶向下”寻找最合适的切分单位：一层一层寻找既能装进区块，又是最高级别的语义单位

   3.5 “贪心”合并，最大化区块长度：分割器就会开始“贪婪地”把相邻的段落一个个合并起来，直到快要超出 500 字符的限制为止

   3.6 它没有使用复杂的机器学习模型去做完美的句子切分，而是用了更简单、更高效的 Unicode 标准方法。这是出于性能考虑，这种“足够好”的方法在绝大多数情况下都能提供一个不错的语义断点，避免了运行缓慢的ML模型.

这些方法都有问题：比如提问某个论文的具体方法实现，根据切分出来的段落，可能只有第一句会提到“method”这个关键词，后面就在具体地讲如何实现的，那么向量化之后method的语义信息就会被稀释，于是很难检索到这部分真正再将method的段落；反而在introduction 或者 conclusion 会反复提到method，作为概述；那么我们需要的method的细节可能就很难被找到
